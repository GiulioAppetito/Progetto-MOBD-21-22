{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVPzZ/a7QaEEYe+Qrgl7H2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiulioAppetito/Progetto_MOBD_2122/blob/main/prova2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn_vjC4iSuRW",
        "outputId": "df97b693-836f-47dc-ead6-a6341e18d4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/ProgettoMOBD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTRrYnQDTEMh",
        "outputId": "98f05be2-c79d-4136-b830-651241351e9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/ProgettoMOBD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn_pandas import CategoricalImputer\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
        "from sklearn import tree\n",
        "import graphviz\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "np.random.seed = 123"
      ],
      "metadata": {
        "id": "xsl9cpj0TdP-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LETTURA DATASET\n",
        "data = pd.read_csv(\"train.csv\", sep=\",\")\n",
        "print(data.head())\n",
        "# print(\"\\nLabels: {}\".format(np.unique(data.Y, return_counts=True)))\n",
        "\n",
        "# print(\"\\n# of NaNs values for each column:\")\n",
        "# print(data.isnull().sum(axis=0))\n",
        "\n",
        "x = data.iloc[:, :-1].to_numpy()\n",
        "y = data.iloc[:, -1].to_numpy()\n",
        "# print(x.shape)\n",
        "# print(y.shape)\n",
        "\n",
        "# stratify: to mantain the same mean as y for y_tr and y_ts\n",
        "x_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size=0.3, random_state=258, stratify=y)\n",
        "\n",
        "# print(x_tr.shape)\n",
        "# print(y_tr.shape)\n",
        "# print(x_ts.shape)\n",
        "# print(y_ts.shape)\n",
        "\n",
        "# print(np.unique(y, return_counts=True)[1]/len(y))\n",
        "# print(np.unique(y_tr, return_counts=True)[1]/len(y_tr))\n",
        "# print(np.unique(y_ts, return_counts=True)[1]/len(y_ts))"
      ],
      "metadata": {
        "id": "2AaafCLmf8zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valori nan\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy = 'most_frequent')\n",
        "x_tr_notnan = imp.fit_transform(x_tr)\n",
        "x_ts_notnan = imp.transform(x_ts)\n",
        "\n",
        "#gestione features categoriche\n",
        "#dato che per√≤ questi sono ndarrays, per fare la one hot coded technique dobbiamo usare dataframes\n",
        "\n",
        "x_tr_df = pd.DataFrame(x_tr_notnan)\n",
        "x_ts_df = pd.DataFrame(x_ts_notnan)\n",
        "\n",
        "x_tr_df.columns=['F0','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13']\n",
        "x_ts_df.columns=['F0','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13']\n",
        "\n",
        "print(x_tr_df.head())\n",
        "print('*******************')\n",
        "#print(x_ts_df.head())\n",
        "\n",
        "# one hot code\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "encoded_df = pd.DataFrame(encoder.fit_transform(x_tr_df[['F1']]).toarray())\n",
        "#print(encoded_df.head())\n",
        "\n",
        "print('*******************')\n",
        "final_df = x_tr_df.join(encoded_df)\n",
        "final_df.rename(columns={0:'K1',1:'K2',2:'K3',3:'K4',4:'K5',5:'K6',6:'K7',7:'K8'}, inplace=True)\n",
        "final_df.drop('F1', axis=1, inplace=True)\n",
        "#print(final_df.head())\n",
        "\n",
        "\n",
        "print('*******************')\n",
        "encoded_df = pd.DataFrame(encoder.fit_transform(x_tr_df[['F7']]).toarray())\n",
        "#print(encoded_df.head())\n",
        "\n",
        "final_df = final_df.join(encoded_df)\n",
        "final_df.rename(columns={0:'M1',1:'M2',2:'M3',3:'M4',4:'M5',5:'M6'}, inplace=True)\n",
        "final_df.drop('F7', axis=1, inplace=True)\n",
        "#print(final_df.head())\n",
        "\n",
        "\n",
        "print('*******************')\n",
        "encoded_df = pd.DataFrame(encoder.fit_transform(x_tr_df[['F9']]).toarray())\n",
        "#print(encoded_df.head())\n",
        "\n",
        "final_df = final_df.join(encoded_df)\n",
        "final_df.rename(columns={0:'Female',1:'Male'}, inplace=True)\n",
        "final_df.drop('F9', axis=1, inplace=True)\n",
        "#print(final_df.head())\n",
        "\n",
        "print('*******************')\n",
        "encoded_df = pd.DataFrame(encoder.fit_transform(x_tr_df[['F8']]).toarray())\n",
        "#print(encoded_df.head())\n",
        "\n",
        "final_df = final_df.join(encoded_df)\n",
        "final_df.rename(columns={0:'American',1:'Asian',2:'Black',3:'Caucasian',4:'Other'}, inplace=True)\n",
        "final_df.drop('F8', axis=1, inplace=True)\n",
        "print(final_df.head(620))\n",
        "\n",
        "\n",
        "# encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "#encoded_df = pd.DataFrame(encoder.fit_transform(final_df[['F3']]).toarray())\n",
        "#print(encoded_df.head())\n",
        "\n",
        "#print('*******************')\n",
        "#final_df = final_df.join(encoded_df)\n",
        "#final_df.rename(columns={0:'R1',1:'R2',2:'R3',3:'R4',4:'R5',5:'R6',6:'R7',7:'R8',8:'R9',9:'R10',10:'R11',11:'R12',12:'R13',13:'R14',14:'R15',15:'R16'}, inplace=True)\n",
        "#final_df.drop('F3', axis=1, inplace=True)\n",
        "#print(final_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ16glfaSw3K",
        "outputId": "4efe5672-99d1-409c-cc31-86fc60d5a1ce"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   F0  F1      F2  F3  F4  F5   F6  F7         F8      F9 F10 F11 F12  F13\n",
            "0  27  K3  130067  R6  10  Q1   P2  M1  caucasian  Female   0   0  65  USA\n",
            "1  18  K3  703067  R3   7  Q1   P6  M4  caucasian    Male   0   0  20  USA\n",
            "2  33  K6  144949  R1  13  Q2   P6  M2  caucasian    Male   0   0  65  USA\n",
            "3  54  K3  176681  R2   9  Q2   P1  M2      black    Male   0   0  20  USA\n",
            "4  39  K3  115076  R6  10  Q2  P10  M2  caucasian    Male   0   0  40  USA\n",
            "*******************\n",
            "*******************\n",
            "*******************\n",
            "*******************\n",
            "*******************\n",
            "     F0      F2  F3  F4  F5   F6 F10 F11 F12  F13  ...   M4   M5   M6  Female  \\\n",
            "0    27  130067  R6  10  Q1   P2   0   0  65  USA  ...  0.0  0.0  0.0     1.0   \n",
            "1    18  703067  R3   7  Q1   P6   0   0  20  USA  ...  1.0  0.0  0.0     0.0   \n",
            "2    33  144949  R1  13  Q2   P6   0   0  65  USA  ...  0.0  0.0  0.0     0.0   \n",
            "3    54  176681  R2   9  Q2   P1   0   0  20  USA  ...  0.0  0.0  0.0     0.0   \n",
            "4    39  115076  R6  10  Q2  P10   0   0  40  USA  ...  0.0  0.0  0.0     0.0   \n",
            "..   ..     ...  ..  ..  ..  ...  ..  ..  ..  ...  ...  ...  ...  ...     ...   \n",
            "615  18   53109  R3   7  Q1   P5   0   0  20  USA  ...  1.0  0.0  0.0     0.0   \n",
            "616  45   76008  R1  13  Q2   P2   0   0  50  USA  ...  0.0  0.0  0.0     0.0   \n",
            "617  20  406641  R6  10  Q1   P1   0   0  35  USA  ...  1.0  0.0  0.0     1.0   \n",
            "618  31  111843  R7  12  Q5   P5   0   0  40  USA  ...  0.0  1.0  0.0     1.0   \n",
            "619  47  362835  R1  13  Q2   P2   0   0  50  USA  ...  0.0  0.0  0.0     0.0   \n",
            "\n",
            "     Male  American  Asian  Black  Caucasian  Other  \n",
            "0     0.0       0.0    0.0    0.0        1.0    0.0  \n",
            "1     1.0       0.0    0.0    0.0        1.0    0.0  \n",
            "2     1.0       0.0    0.0    0.0        1.0    0.0  \n",
            "3     1.0       0.0    0.0    1.0        0.0    0.0  \n",
            "4     1.0       0.0    0.0    0.0        1.0    0.0  \n",
            "..    ...       ...    ...    ...        ...    ...  \n",
            "615   1.0       1.0    0.0    0.0        0.0    0.0  \n",
            "616   1.0       0.0    0.0    0.0        1.0    0.0  \n",
            "617   0.0       0.0    0.0    0.0        1.0    0.0  \n",
            "618   0.0       0.0    0.0    1.0        0.0    0.0  \n",
            "619   1.0       0.0    0.0    0.0        1.0    0.0  \n",
            "\n",
            "[620 rows x 31 columns]\n"
          ]
        }
      ]
    }
  ]
}